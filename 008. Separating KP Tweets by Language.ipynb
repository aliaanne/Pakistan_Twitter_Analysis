{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dval/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import re \n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kp_tweets_cleaned.pickle', \"rb\") as f:\n",
    "    tweets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather twitter-labeled english tweets and delete from rest of dataset\n",
    "twitter_english = tweets[tweets[\"tweet_lang\"]==\"en\"]\n",
    "tweets = tweets[tweets[\"tweet_lang\"] != \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import nltk stopwords and search for additional english tweets using stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "english_stopwords = list(stop_words)\n",
    "def exact_match(stopwords, string):\n",
    "    for word in stopwords:\n",
    "        if re.search(r'\\b' + word + r'\\b', string):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "tweets[\"english\"] = tweets.clean_text.apply(lambda x: exact_match(english_stopwords, x))\n",
    "english_from_stopwords = tweets[tweets[\"english\"]==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dval/.conda/envs/symbolic/lib/python3.5/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "#Combine both sets of english tweets\n",
    "english_tweets = pd.concat([twitter_english, english_from_stopwords], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop any remaining english language tweets from dataset and save english tweets\n",
    "tweets = tweets[tweets[\"english\"]==False]\n",
    "\n",
    "with open(\"kp_tweets_cleaned_english.pickle\", \"wb\") as f:\n",
    "    pickle.dump(english_tweets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather twitter-identified urdu tweets from data and delete from dataset\n",
    "twitter_urdu = tweets[tweets[\"tweet_lang\"]==\"ur\"]\n",
    "tweets = tweets[tweets[\"tweet_lang\"]!=\"ur\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload user-created urdu stopwords and search for them in remaining tweets\n",
    "stop_words = pd.read_excel(\"urdu_stopwords.xlsx\")\n",
    "urdu_stopwords = list(stop_words)\n",
    "tweets[\"urdu\"] = tweets.clean_text.apply(lambda x: exact_match(urdu_stopwords, x))\n",
    "urdu_from_stopwords = tweets[tweets[\"urdu\"]==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get wrongly categorized urdu tweets from data\n",
    "urdu_wrong_cat = tweets[tweets[\"tweet_lang\"]==\"fa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dval/.conda/envs/symbolic/lib/python3.5/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "#Concatenate all urdu-language tweets and export\n",
    "urdu = pd.concat([twitter_urdu, urdu_from_stopwords, urdu_wrong_cat], ignore_index=True)\n",
    "with open(\"kp_tweets_cleaned_urdu.pickle\", \"wb\") as f:\n",
    "    pickle.dump(twitter_urdu, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather twitter identified pashto tweets and remove from dataset\n",
    "twitter_pashto = tweets[tweets[\"tweet_lang\"]==\"ps\"]\n",
    "tweets = tweets[tweets[\"tweet_lang\"]!=\"ps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pashto Arabic Stopwords from #https://github.com/mohbadar/pashto-stopwords/blob/master/stopwords/pashto-stopwords.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather more pashto tweets from stopword list (from url above) and remove from dataset\n",
    "pashto_stopwords = pd.read_excel(\"new_pashto_stopwords.xlsx\")\n",
    "pashto_stopwords = pashto_stopwords[\"word\"].to_list()\n",
    "tweets[\"pashto\"] = tweets.clean_text.apply(lambda x: exact_match(pashto_stopwords, x))\n",
    "pashto_from_stopwords = tweets[tweets[\"pashto\"]==True]\n",
    "tweets = tweets[tweets[\"pashto\"]==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dval/.conda/envs/symbolic/lib/python3.5/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "#Concatente and save pashto language tweets\n",
    "pashto_tweets = pd.concat([twitter_pashto, pashto_from_stopwords], ignore_index=True)\n",
    "with open(\"kp_tweets_pashto.pickle\", \"wb\") as f:\n",
    "    pickle.dump(pashto_tweets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group blank tweets or tweets with fewer than three words - save group of tweets with no or few words and no hashtags\n",
    "tweets[\"clean_text\"] = tweets[\"clean_text\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "tweets[\"text_null\"] = tweets[\"clean_text\"].apply(lambda x: pd.isna(x))\n",
    "tweets[\"hashtags_null\"] = tweets[\"hashtags\"].apply(lambda x: pd.isna(x))\n",
    "blank_tweets = tweets[(tweets.text_null == True) & (tweets.hashtags_null == True)]\n",
    "tweets = tweets[(tweets.text_null == False) | (tweets.hashtags_null == False)]\n",
    "tweets[\"clean_text\"] = tweets.clean_text.apply(lambda x: str(x))\n",
    "tweets[\"clean_text\"] = tweets.clean_text.apply(lambda x: x.lstrip(' '))\n",
    "tweets[\"clean_text\"] = tweets.clean_text.apply(lambda x: x.rstrip(' '))\n",
    "tweets['number_of_words'] = tweets['clean_text'].str.count(' ') + 1\n",
    "tweets[\"few_words\"] = tweets[\"number_of_words\"] < 3\n",
    "few_words_no_hashtag = tweets[(tweets.few_words == True) & (tweets.hashtags_null == True)]\n",
    "tweets = tweets[(tweets.few_words == False) | (tweets.hashtags_null == False)]\n",
    "low_data_tweets = pd.concat([blank_tweets, few_words_no_hashtag], ignore_index=True)\n",
    "with open(\"kp_tweets_no_data.pickle\", \"wb\") as f:\n",
    "    pickle.dump(low_data_tweets, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dval/.conda/envs/symbolic/lib/python3.5/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "#Gather and save blank tweets or tweets with fewer than three words that DO have hashtags\n",
    "hashtags_only1 = tweets[(tweets.text_null == True) & (tweets.hashtags_null == False)]\n",
    "hashtags_only2 = tweets[(tweets.few_words == True) & (tweets.hashtags_null == False)]\n",
    "hashtags_only = pd.concat([hashtags_only1, hashtags_only2], ignore_index=True)\n",
    "with open(\"kp_tweets_hashtags_only.pickle\", \"wb\") as f:\n",
    "    pickle.dump(hashtags_only, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove these low-data tweets from main dataset\n",
    "tweets = tweets.drop(tweets[(tweets.text_null == True) & (tweets.hashtags_null == False)].index)\n",
    "tweets = tweets.drop(tweets[(tweets.few_words == True) & (tweets.hashtags_null == False)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather for romanized urdu tweets using user-created stopword list and remove from dataframe\n",
    "rus = pd.read_excel(\"romanized_urdu_stopwords.xlsx\")\n",
    "rus = rus[\"romanized_urdu\"].to_list()\n",
    "tweets[\"romanized_urdu\"] = tweets.clean_text.apply(lambda x: exact_match(rus, x))\n",
    "romanized_urdu = tweets[tweets[\"romanized_urdu\"]==True]\n",
    "tweets = tweets[tweets[\"romanized_urdu\"]==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather wrongly categorized romanized urdu tweets\n",
    "ru1 = tweets[tweets[\"tweet_lang\"]==\"in\"]\n",
    "ru2 = tweets[tweets[\"tweet_lang\"]==\"hi\"]\n",
    "ru3 = tweets[tweets[\"tweet_lang\"]==\"tl\"]\n",
    "ru4 = tweets[tweets[\"tweet_lang\"]==\"et\"]\n",
    "ru5 = tweets[tweets[\"tweet_lang\"]==\"ht\"]\n",
    "ru6 = tweets[tweets[\"tweet_lang\"]==\"id\"]\n",
    "ru7 = tweets[tweets[\"tweet_lang\"]==\"fi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove wrongly categorized romanized urdu tweets from dataframe\n",
    "tweets = tweets[tweets[\"tweet_lang\"]!=\"in\"]\n",
    "tweets = tweets[tweets[\"tweet_lang\"]!=\"hi\"]\n",
    "tweets = tweets[tweets[\"tweet_lang\"]!=\"tl\"]\n",
    "tweets = tweets[tweets[\"tweet_lang\"]!=\"et\"]\n",
    "tweets = tweets[tweets[\"tweet_lang\"]!=\"ht\"]\n",
    "tweets = tweets[tweets[\"tweet_lang\"]!=\"id\"]\n",
    "tweets = tweets[tweets[\"tweet_lang\"]!=\"fi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat and save romanized urdu tweets\n",
    "romanized_urdu = pd.concat([romanized_urdu, ru1, ru2, ru3, ru4, ru5, ru6, ru7], ignore_index=True)\n",
    "with open(\"kp_tweets_romanized_urdu.pickle\", \"wb\") as f:\n",
    "    pickle.dump(romanized_urdu, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather romanized pashto tweets using user-created stopword list and remove from dataframe and save\n",
    "rps = pd.read_excel(\"romanized_pashto_stopwords.xlsx\")\n",
    "rps = rps[\"romanized_pashto\"].to_list()\n",
    "tweets[\"romanized_pashto\"] = tweets.clean_text.apply(lambda x: exact_match(rps, x))\n",
    "romanized_pashto = tweets[tweets[\"romanized_pashto\"]==True]\n",
    "tweets = tweets[tweets[\"romanized_pashto\"]==False]\n",
    "with open(\"kp_tweets_romanized_pashto.pickle\", \"wb\") as f:\n",
    "    pickle.dump(romanized_pashto, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save all tweets in other unidentified languages\n",
    "with open(\"kp_tweets_other_languages.pickle\", \"wb\") as f:\n",
    "    pickle.dump(tweets, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
