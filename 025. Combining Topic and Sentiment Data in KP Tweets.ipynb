{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dval/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import numpy as np\n",
    "import multiprocessing \n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "# Import required modules\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import collections\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kp_final_hedonometer.pickle', \"rb\") as f:\n",
    "    kp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FINAL_TWEETS_WITH_TOPICS.pickle', \"rb\") as f:\n",
    "     topics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose columns from sentiment analysis to merge with main dataframe\n",
    "sentiments = kp[[\"created_at\", \"hedonometer\", \"hedo_sd\", \"hits\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set index to merge on for both dataframes\n",
    "sentiments = sentiments.set_index(\"created_at\")\n",
    "topics = topics.set_index(\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge dataframes\n",
    "tweets = topics.merge(sentiments, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicates and save\n",
    "tweets = tweets.drop_duplicates([\"clean_translation\",\"user_id\", \"user_location\", \"topic\", \"percent\", \"topic_name\", \"hedonometer\", \"hedo_sd\"], keep=\"first\")\n",
    "\n",
    "with open(\"FINAL_KP_TWEETS_WITH_TOPICS_SENTIMENTS.pickle\", \"wb\") as f:\n",
    "    pickle.dump(tweets, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
