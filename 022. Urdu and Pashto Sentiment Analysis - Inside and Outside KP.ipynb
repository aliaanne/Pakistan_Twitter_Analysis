{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dval/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import numpy as np\n",
    "import multiprocessing \n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "# Import required modules\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload file with google-translated hedonometer words and scores from hedonometer.org (translated by google translate)\n",
    "file = pd.read_csv(\"hedonometer_urdu_pashto.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary of pashto scores\n",
    "scores = file[[\"Pashto\", \"Score\"]]\n",
    "scores.columns = [\"word\", \"score\"]\n",
    "scores = dict(zip(scores.word, scores.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary of pashto standard deviations\n",
    "sds = file[[\"Pashto\", \"Standard Deviation\"]]\n",
    "sds.columns = [\"word\", \"deviation\"]\n",
    "sds = dict(zip(sds.word, sds.deviation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open kp pashto tweets\n",
    "with open('kp_tweets_pashto.pickle', \"rb\") as f:\n",
    "    tweets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula to determine sentiment\n",
    "def sentiment(sentence):\n",
    "    senti=0\n",
    "    total=0\n",
    "    words = sentence.split(\" \")\n",
    "    for word in words:\n",
    "        if word in scores.keys():\n",
    "            senti += scores[word]\n",
    "            total += 1\n",
    "    try:\n",
    "        return senti/total\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define forumula to apply to dataframe\n",
    "def return_sentiment(data):\n",
    "    return pd.Series(data[\"text\"].apply(lambda x: sentiment(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply formula to parallelize across cores\n",
    "def parallelize_dataframe(df, func, n_cores=25):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate pashto sentiment\n",
    "tweets[\"hedonometer\"] = parallelize_dataframe(tweets, return_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula to calculate sd of scores\n",
    "def standard_deviation(sentence):\n",
    "    standard_deviation=0\n",
    "    total=0\n",
    "    words = sentence.split(\" \")\n",
    "    for word in words:\n",
    "        if word in sds.keys():\n",
    "            standard_deviation += sds[word]\n",
    "            total += 1\n",
    "    try:\n",
    "        return standard_deviation/total\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula to apply to dataframe\n",
    "def return_standard_deviation(data):\n",
    "    return pd.Series(data[\"text\"].apply(lambda x: standard_deviation(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula to parallelize across cores\n",
    "def parallelize_dataframe(df, func, n_cores=25):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate sd of pashto tweets\n",
    "tweets[\"hedo_sd\"] = parallelize_dataframe(tweets, return_standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a formula to select the words used to define sentiment\n",
    "def grab_hits(sentence):\n",
    "    hits=[]\n",
    "    words = sentence.split(\" \")\n",
    "    for word in words:\n",
    "        if word in sds.keys():\n",
    "            hits.append(word)\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula to apply it to dataframe\n",
    "def return_hits(data):\n",
    "    return pd.Series(data[\"text\"].apply(lambda x: grab_hits(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula to parallelize across cores\n",
    "def parallelize_dataframe(df, func, n_cores=25):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab words used to determine sentiment and save in column\n",
    "tweets[\"hits\"] = parallelize_dataframe(tweets, return_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save file\n",
    "with open(\"NEW_non_kp_hedonometer_pashto.pickle\", \"wb\") as f:\n",
    "    pickle.dump(tweets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPEAT FOR KP URDU, NON-KP PASHTO AND NON-KP URDU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
