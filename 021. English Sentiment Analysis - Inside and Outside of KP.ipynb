{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dval/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import numpy as np\n",
    "import multiprocessing \n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "# Import required modules\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Set charts to view inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in hedonometer words from hedonometer.org and create dictionary of words and scores\n",
    "file = pd.read_csv(\"Hedonometer.csv\")\n",
    "scores = file[[\"Word\", \"Happiness Score\"]]\n",
    "scores.columns = [\"word\", \"score\"]\n",
    "scores = dict(zip(scores.word, scores.score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using same uploaded file create dictionary of words and standard deviations of scores\n",
    "sds = file[[\"Word\", \"Standard Deviation of Ratings\"]]\n",
    "sds.columns = [\"word\", \"deviation\"]\n",
    "sds = dict(zip(sds.word, sds.deviation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open kp tweets\n",
    "with open('kp_tweets_cleaned_english.pickle', \"rb\") as f:\n",
    "    tweets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula for determining sentiment of a tweet\n",
    "def sentiment(sentence):\n",
    "    senti=0\n",
    "    total=0\n",
    "    words = sentence.split(\" \")\n",
    "    for word in words:\n",
    "        if word in scores.keys():\n",
    "            senti += scores[word]\n",
    "            total += 1\n",
    "    try:\n",
    "        return senti/total\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula applying sentiment formulat to dataframe\n",
    "def return_sentiment(data):\n",
    "    return pd.Series(data[\"clean_text\"].apply(lambda x: sentiment(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula to parallelize the code on multiple cores\n",
    "def parallelize_dataframe(df, func, n_cores=25):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dval/.conda/envs/symbolic/lib/python3.5/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "#Run the hedonometer on the tweets\n",
    "tweets[\"hedonometer\"] = parallelize_dataframe(tweets, return_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function calcualting standard deviation of a tweets sentiment\n",
    "def standard_deviation(sentence):\n",
    "    standard_deviation=0\n",
    "    total=0\n",
    "    words = sentence.split(\" \")\n",
    "    for word in words:\n",
    "        if word in sds.keys():\n",
    "            standard_deviation += sds[word]\n",
    "            total += 1\n",
    "    try:\n",
    "        return standard_deviation/total\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula applying that to a dataframe\n",
    "def return_standard_deviation(data):\n",
    "    return pd.Series(data[\"clean_text\"].apply(lambda x: standard_deviation(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula to parallelize the code on multiple cores\n",
    "def parallelize_dataframe(df, func, n_cores=25):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dval/.conda/envs/symbolic/lib/python3.5/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "#Calcualte the sd for each tweets sentiment\n",
    "tweets[\"hedo_sd\"] = parallelize_dataframe(tweets, return_standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a formula to grab the words used to determine the sentiment of a tweet\n",
    "def grab_hits(sentence):\n",
    "    hits=[]\n",
    "    words = sentence.split(\" \")\n",
    "    for word in words:\n",
    "        if word in sds.keys():\n",
    "            hits.append(word)\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a forumula to apply this to a dataframe\n",
    "def return_hits(data):\n",
    "    return pd.Series(data[\"clean_text\"].apply(lambda x: grab_hits(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a formula to parallelize function on multiple cores\n",
    "def parallelize_dataframe(df, func, n_cores=25):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-157:\n",
      "Process ForkPoolWorker-168:\n",
      "Process ForkPoolWorker-151:\n",
      "Process ForkPoolWorker-152:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dval/.conda/envs/symbolic/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dval/.conda/envs/symbolic/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dval/.conda/envs/symbolic/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dval/.conda/envs/symbolic/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dval/.conda/envs/symbolic/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dval/.conda/envs/symbolic/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/dval/.conda/envs/symbolic/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "#Run funciton to pull the words from each tweet that determine sentiment into a new column\n",
    "tweets[\"hits\"] = parallelize_dataframe(tweets, return_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save english language kp tweets with sentiments\n",
    "with open(\"NEW_kp_hedonometer_english\", \"wb\") as f:\n",
    "    pickle.dump(tweets, f)\n",
    "    \n",
    "#WHEN DONE - REPEAT ON NON-KP ENGLISH TWEETS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
